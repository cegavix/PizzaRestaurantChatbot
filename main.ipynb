{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T20:57:59.588631Z",
     "start_time": "2023-11-10T20:57:59.368471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resource: wordnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/julia.clegg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, ne_chunk, pos_tag\n",
    "import csv\n",
    "\n",
    "def set_name(user_input):\n",
    "    # TODO: If u type hello, it will take this as the name, or just the first word!!! fix? ne_chunks? ngrams? (look for (name, NNP) tuples\n",
    "    # Use library to perform entity recognition using bag of words tagging\n",
    "    name = \"NOT FOUND\"\n",
    "    \n",
    "    # Pinpoint which word is the name in input:\n",
    "    pos_tags = nltk.pos_tag(word_tokenize(user_input))\n",
    "    for entity in pos_tags:\n",
    "        if isinstance(entity, tuple) and entity[1] == 'NNP': # NNP: Proper noun, singular\n",
    "            name = entity[0]\n",
    "            print(\"NNP Name found: %s\"%entity[0])\n",
    "            #Once ideal name is found, the system stops looking\n",
    "            return name\n",
    "        elif isinstance(entity, tuple) and entity[1] == 'NN':\n",
    "            name = entity[0]\n",
    "            print(\"Name found: %s\"%entity[0])\n",
    "        elif len(pos_tags) == 1:\n",
    "            name = entity[0]\n",
    "    return name\n",
    "    \n",
    "\n",
    "\n",
    "def calculate_intent(query, threshold, intents):\n",
    "     # See if query matches an intent \n",
    "\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,3),analyzer='word')\n",
    "    X_counts = tfidf.fit_transform(intents)\n",
    "    Y_counts = tfidf.get_feature_names_out(X_counts)\n",
    "    input_tfidf = tfidf.transform([query.lower()]).toarray()\n",
    "    cosine_similarities = cosine_similarity(input_tfidf, X_counts)\n",
    "    if cosine_similarities.max() >= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# make a document with all the intent pointers labelled with the intent\n",
    "# train vectorizer or classifier on this \n",
    "name_intents = ['my name is','i am called','rename','call me','change my name']\n",
    "booking_intents = [\"can i book\", \"make a reservation\", \"reserve\", \"booking\",\"appointment\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c80d9b8eee76dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T20:58:40.427989Z",
     "start_time": "2023-11-10T20:58:25.740141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name found: name\n",
      "NNP Name found: Constance\n",
      "Papa: Hello Constance, it is nice to meet you! How can I help you today?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_intent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39muser_name)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#if one word Noun response -> isname\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcalculate_intent\u001B[49m(user_input, \u001B[38;5;241m0.9\u001B[39m, name_intents):\n\u001B[1;32m     16\u001B[0m     user_name \u001B[38;5;241m=\u001B[39m set_name(user_input)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m calculate_intent(user_input,\u001B[38;5;241m0.5\u001B[39m,booking_intents):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'calculate_intent' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "# Does this seem more like classifier? LOgistic regression for intent, TfIDF for Question answering (NLP search engine)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_name = 'User'\n",
    "    flag = True   \n",
    "    user_input = input(\"Hi, I'm Papa the Chatbot for Papa's Pizzeria! And what is your name?\")\n",
    "    user_name = set_name(user_input)\n",
    "    print(\"Papa: Hello %s, it is nice to meet you! How can I help you today?\" %user_name)\n",
    "    \n",
    "    while flag:\n",
    "        user_input = input('%s: ' %user_name)\n",
    "        #if one word Noun response -> is name \n",
    "        if calculate_intent(user_input, 0.9, name_intents):\n",
    "            user_name = set_name(user_input)\n",
    "    \n",
    "        if calculate_intent(user_input,0.5,booking_intents):\n",
    "            print('Papa: Redirecting you to the booking personnel....')\n",
    "    \n",
    "        \n",
    "        if user_input == 'q':\n",
    "            flag = False\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad818c1e7a1151",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# def extract_names_with_keywords(text, keyword, n=3):\n",
    "#     # Tokenize the text\n",
    "#     tokens = word_tokenize(text.lower())  # Convert to lowercase for case-insensitivity\n",
    "# \n",
    "#     # Generate n-grams\n",
    "#     n_grams = list(ngrams(tokens, n))\n",
    "# \n",
    "#     # Extract names following the specified keyword\n",
    "#     names = []\n",
    "#     for i, gram in enumerate(n_grams):\n",
    "#         if keyword in gram:\n",
    "#             # Extract the word following the keyword\n",
    "#             name_candidate = gram[gram.index(keyword) + 1]\n",
    "#             names.append(name_candidate)\n",
    "# \n",
    "#     return names\n",
    "# \n",
    "# text = \"My name is John. What is your name?\"\n",
    "# keyword = \"name is\"\n",
    "# names = extract_names_with_keywords(text, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd3dcf4f32b143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
